# Exploring Sparse Reward Environments with Weight Agnostic Neural Networks

![Python](https://img.shields.io/badge/python-3.8-green.svg)

## ğŸš€ Overview
Sparse reward environments present a significant challenge in reinforcement learning (RL), as agents receive little to no feedback for extended periods, making effective learning difficult. Traditional RL algorithms struggle in these settings without human-engineered feedback to guide training.

### **Why Sparse Rewards Matter?**
Many real-world RL applications provide only sparse rewards, requiring the algorithm to find a "needle in the haystack" solution. Challenges include:
- **Delayed Feedback:** Agents receive rewards only upon completing a task, making it hard to assign credit.
- **Exploration Difficulty:** Standard RL approaches struggle to find rare trajectories that lead to rewards without guidance.

<p align="center">
  <img src="img/sparse_landscape.png" alt="Sparse Optimization Landscape" width="400">
</p>

### **Our Approach: WANNs for Sparse RL**
We explore a novel direction using **Weight Agnostic Neural Networks (WANNs)**, which leverage evolutionary search to discover network architectures for solving such tasks.

We evaluate WANNs on modified versions of the **MountainCar** and **LunarLander** environments, where rewards are only given upon successful task completion. Our results demonstrate that WANNs can successfully learn compact, interpretable policies in these settings, whereas conventional RL methods fail without reward shaping!

---
## ğŸ“Š Results
### **Discovered WANN Network and Policy**
Example solution: the best WANN model for the discrete Sparse Mountain Car (SMC) task learns an effective and interpretable policy:

<p align="center">
  <img src="img/example_solution.png" alt="WANN Network, Policy Visualization, MountainCar Task" width="1200">
</p>

### **Performance Comparison**
| Method     | SMC Discrete | SMC Continuous | Lunar Lander |
|------------|-------------|----------------|--------------|
| **WANN**   | **123.92**  | **136.73**     | **1135.37**  |
| Q-Learning | âˆ (110.53)  | âˆ (âˆ)          | âˆ            |
| PPO        | âˆ (133.27)  | âˆ (224.25)     | âˆ            |
| DQN        | âˆ (322.79)  | âœ—              | âˆ            |

*Average time steps to reach the goal;âˆ denotes failure to reach the goal; values in parentheses indicate performance with reward shaping applied.*

---

## ğŸ“Œ Key Takeaways
âœ… **WANNs succeed where standard RL fails** in sparse environments.  
âœ… **No reward shaping required**, reducing manual effort.  
âœ… **Compact, interpretable networks** discovered via evolutionary search.  

---
## ğŸ“‚ Directory Structure
```
.
â”œâ”€â”€ wann_train.py        # Evolutionary search for networks solving the task
â”œâ”€â”€ wann_test.py         # Evaluation and visualization of trained WANNs
â”œâ”€â”€ visualizer.py        # Network structure and policy visualization tool
â”œâ”€â”€ pareto_front.py      # Displays the Pareto front (fitness vs. complexity)
â”œâ”€â”€ Sparse-RL-WANN.pdf   # Project report summary
â”‚
â”œâ”€â”€ wann_src/            # Helper functions for WANN EA process
â”œâ”€â”€ p/                   # JSON config files for experiment parameters
â”œâ”€â”€ domain/              # Task environments
â”œâ”€â”€ champions/           # Best evolved models stored here
â”œâ”€â”€ RL/                  # PPO, DQN, and Q-Learning implementations
â”‚
â””â”€â”€ requirements.txt      # Required dependencies
```

---

## ğŸ›  Installation
Clone this repository and install dependencies:
```bash
git clone https://github.com/Tobi-Tob/Sparse-RL-Wann.git
cd Sparse-RL-Wann
pip install -r requirements.txt
```

---

## ğŸƒ Running Experiments
### 1ï¸âƒ£ Train a WANN
Run evolutionary search to discover a weight-agnostic network for a given environment:
```bash
python wann_train.py
```

### 2ï¸âƒ£ Test and Visualize WANN Policies
Evaluate a trained WANN model on an environment:
```bash
python wann_test.py
```
Visualize the discovered network structure and policy decisions:
```bash
python visualizer.py
```

This project builds on the original WANN framework from [Google Brain Tokyo Workshop](https://github.com/google/brain-tokyo-workshop/tree/master/WANNRelease).
